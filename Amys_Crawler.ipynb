{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf579abbdaa5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def print_welcome_message():\n",
    "    welcome_message = \"\"\"\n",
    "    🌐✨ Hello and Welcome to Amy's Crawler! ✨🌐\n",
    "    This crawler will search Wikipedia, arXiv, Google Scholar, and PubMed for a specified keyword.\n",
    "    It will index the content of pages and search through them efficiently. 🚀\n",
    "    \"\"\"\n",
    "    print(welcome_message)\n",
    "\n",
    "def handle_http_errors(status_code, source, retries=1):\n",
    "    if status_code == 403:\n",
    "        return f\"🚫 Access to {source} is restricted (403 Forbidden).\"\n",
    "    elif status_code == 404 and retries > 0:\n",
    "        print(f\"🔄 Retrying {source} due to 404 error...\")\n",
    "        time.sleep(2)\n",
    "        return None  \n",
    "    elif status_code == 404:\n",
    "        return f\"❌ The page on {source} was not found (404 Not Found).\"\n",
    "    elif status_code == 429 and retries > 0:  \n",
    "        wait_time = random.uniform(10, 20)  \n",
    "        print(f\"⚠️ Rate limiting detected (429) for {source}. Retrying in {wait_time:.2f} seconds...\")\n",
    "        time.sleep(wait_time)  \n",
    "        return None  \n",
    "    elif status_code == 500:\n",
    "        return f\"⚠️ {source} is experiencing internal issues (500 Server Error).\"\n",
    "    elif status_code == 503:\n",
    "        return f\"🔧 {source} is temporarily unavailable (503 Service Unavailable).\"\n",
    "    else:\n",
    "        return f\"⚠️ An error occurred while accessing {source} (Status Code: {status_code}).\"\n",
    "\n",
    "\n",
    "def crawl_and_index(keyword, max_depth=1, max_links_per_site=20):\n",
    "    base_urls = {\n",
    "        \"Wikipedia\": \"https://en.wikipedia.org/w/index.php?search=\",\n",
    "        \"arXiv\": \"https://arxiv.org/search/?query=\",\n",
    "        \"Google Scholar\": \"https://scholar.google.com/scholar?hl=en&q=\",\n",
    "        \"PubMed\": \"https://pubmed.ncbi.nlm.nih.gov/?term=\",\n",
    "    }\n",
    "\n",
    "    visited_links = set()  \n",
    "    keyword_occurrences = [] \n",
    "    related_links = {}  \n",
    "    index = defaultdict(list)  \n",
    "\n",
    "    def recursive_crawl(current_url, keyword, depth, retries=1):\n",
    "        if depth > max_depth or current_url in visited_links:\n",
    "            return\n",
    "        visited_links.add(current_url)\n",
    "\n",
    "        print(f\"🔍 Crawling URL: {current_url}\")  \n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "\n",
    "            \n",
    "            if response.status_code == 404 and retries > 0:\n",
    "                error_message = handle_http_errors(response.status_code, current_url, retries=retries)\n",
    "                if error_message:\n",
    "                    print(error_message)\n",
    "                    return\n",
    "                return recursive_crawl(current_url, keyword, depth, retries - 1)\n",
    "            elif response.status_code == 429 and retries > 0: \n",
    "                error_message = handle_http_errors(response.status_code, current_url, retries=retries)\n",
    "                if error_message:\n",
    "                    print(error_message)\n",
    "                    return\n",
    "                return recursive_crawl(current_url, keyword, depth, retries - 1)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"✅ Successfully fetched: {current_url}\")  \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                page_text = soup.get_text()\n",
    "                title = soup.find('title').text.strip() if soup.find('title') else \"No Title\"\n",
    "                keyword_count = page_text.lower().count(keyword.lower())\n",
    "\n",
    "                \n",
    "                words = re.findall(r'\\w+', page_text.lower())  \n",
    "                for word in set(words):  \n",
    "                    index[word].append(current_url)\n",
    "\n",
    "                if keyword_count > 0:\n",
    "                    keyword_occurrences.append({\n",
    "                        'url': current_url,\n",
    "                        'title': title,\n",
    "                        'keyword_count': keyword_count\n",
    "                    })\n",
    "\n",
    "                \n",
    "                if current_url not in related_links:\n",
    "                    related_links[current_url] = []\n",
    "\n",
    "                \n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    if len(related_links[current_url]) >= max_links_per_site:\n",
    "                        break\n",
    "                    href = link['href']\n",
    "                    full_link = href if href.startswith('http') else re.sub(r'/$', '', current_url) + '/' + href.lstrip('/')\n",
    "\n",
    "                    if keyword.lower() in link.text.lower():\n",
    "                        related_links[current_url].append({\n",
    "                            'url': full_link,\n",
    "                            'title': link.text.strip()\n",
    "                        })\n",
    "\n",
    "                \n",
    "                return\n",
    "\n",
    "            else:\n",
    "                print(handle_http_errors(response.status_code, current_url, retries=retries))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Request failed for {current_url}: {e}\")\n",
    "            time.sleep(random.uniform(1, 3))  \n",
    "\n",
    "    \n",
    "    for site, url_prefix in base_urls.items():\n",
    "        if site == \"arXiv\":\n",
    "            search_url = f\"{url_prefix}{keyword.replace(' ', '+')}&searchtype=all&abstracts=show&order=-announced_date_first&size=50\"\n",
    "        elif site == \"Wikipedia\":\n",
    "            search_url = f\"{url_prefix}{keyword.replace(' ', '%2Fwiki%2F')}\"\n",
    "        else:\n",
    "            search_url = f\"{url_prefix}{keyword.replace(' ', '+')}\"\n",
    "        \n",
    "        print(f\"🚀 Starting crawl on {site} with URL: {search_url}\")\n",
    "        recursive_crawl(search_url, keyword, 0)\n",
    "\n",
    "    return keyword_occurrences, related_links, index\n",
    "\n",
    "\n",
    "def search_index(keyword, index):\n",
    "    print(f\"\\n🔍 Searching for '{keyword}' in the index...\\n\")\n",
    "\n",
    "    if keyword.lower() in index:\n",
    "        results = index[keyword.lower()]\n",
    "        print(f\"🔑 Found the keyword in {len(results)} pages:\")\n",
    "        for result in results:\n",
    "            print(f\"  - 🗂️ {result}\")\n",
    "    else:\n",
    "        print(f\"❌ No results found for the keyword '{keyword}' in the index.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print_welcome_message()\n",
    "    keyword = input(\"🔍 Enter the keyword to search for: \").strip()\n",
    "\n",
    "    keyword_occurrences, related_links, index = crawl_and_index(keyword)\n",
    "\n",
    "    print(\"\\n--- 📝 Crawler Results ---\")\n",
    "\n",
    "    print(\"\\n🔍 **Keyword Occurrences**:\")\n",
    "    for entry in keyword_occurrences:\n",
    "        print(f\"🗂️ **URL**: {entry['url']}\\n📋 **Title**: {entry['title']}\\n🔑 **Keyword Count**: {entry['keyword_count']}\\n\")\n",
    "\n",
    "    print(\"\\n🔗 **Related Links (Keyword in Link Text, up to 20 per site)**:\")\n",
    "    for page, links in related_links.items():\n",
    "        print(f\"\\n📄 **Page**: {page}\")\n",
    "        for link in links:\n",
    "            print(f\"  - 📝 **Title**: {link['title']}\\n    🔗 **Link**: {link['url']}\")\n",
    "\n",
    "    \n",
    "    search_index(keyword, index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
